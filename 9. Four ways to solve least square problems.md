# 9. Four ways to solve least square problems

## Pseudo inverse

$$
matrix\ A: \ m\times n\\
pseudo\ inverse\ A^+: \ n\times m
$$

If $A^{-1}$ exists, (requires full rank, $AA^{-1}=A^{-1}A=I$) , then $A^+=A^{-1}$.

When the matrix is rectangular, or it has zero eigenvalues(the columns are dependent),  how to inverse it ?  Since $A^{-1}Ax=0$.

We gotta find a pseudo inverse $A^+$ that satisfy such projection in fundamental subspace.

<img src="9. Four ways to solve least square problems.assets/image-20211220153952646.png" alt="image-20211220153952646" style="zoom:40%;" />

$A^+A$ can be seen as a projection as it put $x$ in row space to where it from, and $x$ in null space to $0$.

## Formula

Start from SVD, since the SVD works for any matrix.
$$
A = U\Sigma V^T
$$
where $\Sigma$  is singular value matrix (eigenvalues of $A^TA$ )

### If invertible ...

If A has an inverse, diagonal would not include 0s.
$$
\Sigma_{n\times n}=\begin{bmatrix}
\sigma_1 &  &  &  \\
 & \sigma_2 &  & \\
 &  & ...&  \\
 & & & \sigma_n \end{bmatrix}
$$
Then SVD of $A^{-1}$
$$
A^{-1} = V\Sigma^{-1} U^T
$$


### If not invertible ...

In the case A has two nonzero singular values
$$
\Sigma_{n\times m}=\begin{bmatrix}
\sigma_1 &  &  & &  \\
 & \sigma_2 &  & 0\\
   & &  \\
 &0 & & 0 \end{bmatrix}
$$
The SVD of $A^+$
$$
A^{+} = V\Sigma^+ U^T
$$
What should $\Sigma^+$ be like?
$$
\Sigma^+_{m\times n}=\begin{bmatrix}
\frac{1}{\sigma_1} &  &  &  &\\
 &\frac{1}{\sigma_2} &  &0 \\
 &  & &  \\
 & 0& &0  \end{bmatrix}
$$

$$
(\Sigma^+\Sigma)_{m\times m}=\begin{bmatrix}
1 &  &  &  &\\
 &1 &  &0 \\
 &  & &  \\
 & 0& &0  \end{bmatrix}
$$

## What is Least Squares Problems?

$$
A_{m\times n}x=b\\ where\ A\ is\ not\ invertible,\ rank=r
$$

When $m=n=r$, solution is
$$
x = A^{-1}b
$$
When $r<m$ and $r<n$ , we don't have a solution.

A classical example of the least squares problems is $fit\ straight\ line.$

<img src="9. Four ways to solve least square problems.assets/image-20211220164855228.png" alt="image-20211220164855228" style="zoom:40%;" />


$$
Ax =\begin{bmatrix}
1 & x_1 \\
1 & x_2\\
... &  ...\\
1 & x_m \end{bmatrix}
\begin{bmatrix}
C\\
D\\
 \end{bmatrix}=
\begin{bmatrix}
b_1\\
b_2\\
...\\
b_m
 \end{bmatrix} = b
$$
Because noisy set of measurements, we won't hint the solution right on the line.

What should we do when there is no solution?

### The ordinary run of the least squares problems

We'll do what Gauss do... to minimize the 2 norm errors.
$$
minimize\ ||Ax-b||^2 = (Ax-b)^T(Ax-b)\\
=x^TA^TAx-2b^TAx+b^Tb
$$
This is a linear regression, not of course just for straight line fits.

What we minimize actually lead to
$$
A^TA\hat x=A^Tb
$$
Gauss assumes that A has independent columns, so it works.

<img src="9. Four ways to solve least square problems.assets/image-20211220214500718.png" alt="image-20211220214500718" style="zoom:40%;" />

When we look back the equation
$$
Ax =\begin{bmatrix}
1 & x_1 \\
1 & x_2\\
... &  ...\\
1 & x_m \end{bmatrix}
\begin{bmatrix}
C\\
D\\
 \end{bmatrix}=
\begin{bmatrix}
b_1\\
b_2\\
...\\
b_m
 \end{bmatrix} = b
$$
A has independent row space, just like

<img src="9. Four ways to solve least square problems.assets/image-20211220222234512.png" alt="image-20211220222234512" style="zoom:40%;" />

We have such claim.
$$
When\ N(A)=0\ (or\ r=n)\\
Claim\ A^+b=(A^TA)^{-1}A^Tb
$$

$$
A^+=V\Sigma^+U^T=(A^TA)^{-1}A^T
$$

$N(A)=0$ means $A^TA$ is veritable while $AA^T$ not.

> Notice that
> $$
> (A^TA)^{-1}A^TA=I\\
> A(A^TA)^{-1}A^T\ne I
> $$

## Gram-Schmidt

Orthogonal first would be much more easy, a boring way.

Orthogonal step is projection, a natural way.

