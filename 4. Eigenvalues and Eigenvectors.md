# 4 Eigenvalues and Eigenvectors

Final purpose of this lecture is about Symmetric Matrices: we'll start from arbitrary matrix $A$ to `symmetric matrix` $S$ then to matrix's `Positive Definite` property.

For $A\in \mathbb{R}^{n\times n}$, eigenvalue $\lambda_i$ and eigenvector $x_i$ satisfies:
$$
Ax_i = \lambda_i x_i{\ \ \ \ }i=1,\dots,n
$$
Why eigenvectors are important?

* $f(A)$ has same eigenvectors as $A$

* $A^nx = \lambda^nx$

* $e^{At}x = e^{\lambda t}x$

* $A^{-1}x = \frac{1}{\lambda}x$ (If $\lambda = 0$, then $Ax=0$, $x$ is in A's null space, $A$ is not invertible)

  >  If there exists x to make Ax = 0, then A is not invertible. (A couldn't have null space unless the determinant is 0)

Why they are useful?——evectors are invented to solve `difference equations`

Suppose $A$ has $n$ independent eigenvectors, for any vector $v$:

* $v = c_1x_1 + c_2x_2 + \dots + c_nx_n$
* $v_k = A^kv = c_1\lambda_1^kx_1 + \dots + c_n\lambda_n^kx_n$

The difference equations:

* $v_{k+1} = Av_k$, solution for $v$: $c_1x_1 + c_2x_2 + \dots + c_nx_n$
* $\frac{dv}{dt} = Av$, solution: $c_1e^{\lambda_1t}x_1+\dots+c_ne^{\lambda_nt}x_n$

The solution is trivial if we know eigenvectors.

## Similar Matrices

We say $B$ is similar to $A$ means <font color='cornflowerblue'>$B = M^{-1}AM$</font>, $M$ is an invertible matrix.

$A, B$ have <font color='cornflowerblue'>same eigenvalues</font> but different eigenvectors——a very useful point. 
$$
M^{-1}AMy = \lambda y\\ A(My) = \lambda (My)
$$
A <font color='cornflowerblue'>family</font> of similar matrices have same eigenvalues, and the simplest of them（标准形） is a triangular matrix. For symmetric matrix, the simplest is a diagonal matrix with eigenvalues standing on its diagonal. （一族相似矩阵中最简单的那个就是<font color='red'>标准形</font>！！！）

$AB$ has the same non-zero eigenvalues with $BA$ for any $A, B$: $AB$ is similar to $BA$, where $M=B$

Evalues of (A+B) $\ne$ Evalue of A + Evalue of B.

## Symmetric Matrix

For real symmetric matrix $S$, its evalues are all real, while others could have imaginary evalues.

### Anti-symmetric

For a 90 degree rotation matrix $A$, it's anti-symmetric ($A^T = A^{-1}$):
$$
A = \begin{bmatrix}
0 & 1\\
-1 & 0
\end{bmatrix}
$$
There is no way for $Ax$ to be a multiple of $x$, so there's no real eigenvector:

<img src="4. Eigenvalues and Eigenvectors.assets/image-20211211214936432.png" alt="image-20211211214936432" style="zoom:50%;" />

$A's$ evalues are $i, -i$. 

Two useful tricks to check evalues:

* Sum of all evalues is equal to the trace
* $\prod_i\lambda_i = {\rm det} A$

### Symmetric

Symmetric matrix's evalues are **real**, evectors are <font color='red'>**orthogonal**</font>.

**How the evectors pay off:** 

we can combine them into $M$ and get a similar matrix $\Lambda$ which is diagonal, whose diagonal elements are evalues.

**Examples:**

For $S=
\begin{bmatrix}
0 & 1\\
1 & 0
\end{bmatrix}$, its evalues $\lambda = 1, -1$, evectors $x = \begin{bmatrix}1\\1\end{bmatrix},\begin{bmatrix}1\\-1\end{bmatrix}$.

Then the combination of evectors gives $M = 
\begin{bmatrix}
1 & 1\\
1 & -1
\end{bmatrix}$, which can make $S$ similar to a diagonal matrix $\Lambda$, whose diagonal elements are evalues:
$$
M^{-1}SM = 
\begin{bmatrix}
1 & 0\\
0 & -1
\end{bmatrix} = \Lambda
$$
proof:
$$
SM = M
\begin{bmatrix}
1 & 0\\
0 & -1
\end{bmatrix}\\
[Sx_1, Sx_2] = [x_1, -x_2]
$$

## Conclusion

For matrix $A$ who has evalues $\lambda_1, \dots, \lambda_n$ and evectors $x_1, \dots, x_n$, then
$$
A
\begin{bmatrix}
| &  & | \\
x_1 & ... & x_n \\
| &  & |
\end{bmatrix}
= \begin{bmatrix}
| &  & | \\
x_1 & ... & x_n \\
| &  & |
\end{bmatrix}
\begin{bmatrix}
\lambda_1 &  & \\
 & ... &  \\
 &  & \lambda_n
\end{bmatrix}
\\
AX = X\Lambda \Longrightarrow A = X\Lambda X^{-1}\\
A^2 = X\Lambda^2 X^{-1}
$$

#### Spectral Theorem:

For Symmetric Matrix, evectors are orthogonal. We normalize all evectors then $X$ becomes an orthogonal matrix: 
$$
S = Q\Lambda Q^{-1} = Q\Lambda Q^{T}
$$
It tells us what every symmetric matrix looks like.